{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoSQL - HBase\n",
    "\n",
    "## What is HBase?\n",
    "\n",
    "> HBase is an open-source, column-oriented distributed data store that runs typically in a Hadoop environment. \n",
    "\n",
    "Hbase is a tool used to store and access massive amounts of data. Although it can handle structured data, Hbase is designed mainly to efficiently manage semi-structured and unstructred data types that a traditional relational database couldn't handle.  Hbase was originally developed by Google and was called Big Table. Afterwards, it was renamed to HBase and became a project under the Apache foundation.  Apache HBase is typically used for _real-time_ big data applications. HBase can store and interact with massive amounts of data (terabytes to petabytes) which are stored in _table_ structures. The tables present in HBase can consist of billions of rows having millions of columns. HBase is built for low latency operations, which provides benefits compared to traditional relational models.\n",
    "\n",
    "For an introductory video on HBase, check out this Huwaei lecture:\n",
    "- [Introduction to HBase](https://www.youtube.com/embed/VUkPIT97J9A)\n",
    "\n",
    "### HBase Architecture\n",
    "\n",
    "> HBase can run independently in a stand-alone mode, or in a distributed environment running on top of Hadoop's HDFS (in a pseudo-distributed or fully distributed mode).\n",
    "\n",
    "Hbase's flexible architecture allow the tool to be installed independently, in a mode called stand-alone.  This is mainly used for testing and proof of concept purposes. However, in real production environments, HBase is normally integrated with Hadoop to leverage HDFS as the back-end storage repository.\n",
    "\n",
    "The main storage entity in Hbase is a _table_, which consist of rows and columns.  The intersection of a row and column is called a _cell_, which stores data. Tables are sorted by the row.  Table schemas are defined using something called a __column family__, whereby each column family can have any number of columns associated with it.  Each column is a collection of _key value_ pairs.\n",
    "\n",
    "To summarize, in HBase:\n",
    "\n",
    "- Table - is a collection of rows\n",
    "- Row - is a collection of column families\n",
    "- Column family - is a collection of columns\n",
    "- Column - is a collection of key-value pairs\n",
    "- Regions - tables are split into regions, with each region storing a \"range\" of rows.  They store the data in HDFS.\n",
    "- Region server - this server communicates with the user of the system and oversees a group of regions.  It coordinates all read/write data related requests to the regions under its command.\n",
    "\n",
    "Below is a visual representation of a typical HBase table:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-table-schema.png\" width=600>\n",
    "</p>\n",
    "\n",
    "\n",
    "HBase consists of 3 main components:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-architecture.png\" width=600>\n",
    "</p>\n",
    "\n",
    "\n",
    "__1. HMaster__\n",
    "\n",
    "HMaster represents the master server in Hbase.  Mainly, the master handles task assignment, network load balancing and cluster operations.  To be more specific, the main responsibilities of the master include:\n",
    "\n",
    "-   Assigns regions to the region servers (uses help from Apache ZooKeeper for this task).\n",
    "-   Handles load balancing of the regions across region servers. It unloads the busy servers and shifts the regions to less occupied servers.\n",
    "-   Is responsible for schema changes and other metadata operations such as creation of tables and column families.\n",
    "\n",
    "__2. Region Server__\n",
    "\n",
    "HBase tables are divided horizontally by the _row key_ into regions. _Regions_ are simply Hbase tables split up and spread across a distributed network called _region servers_.  This split is done for performance and data reliability reasons.  \n",
    "\n",
    "The region servers have regions under their control that:\n",
    "-   Communicate with the client and handle data-related operations.\n",
    "-   Handle read and write requests.\n",
    "-   Decide the size of the region by following the region size thresholds.\n",
    "\n",
    "__3. Zookeeper__\n",
    "\n",
    "Zookeeper is an additional component that works as a coordinator for HBase, especially when HDFS is used to store the data. Zookeeper is an open-source Apache project that provides services like maintaining configuration information, naming, and providing distributed synchronization. Some of the main tasks include:\n",
    "\n",
    "-   Zookeeper has ephemeral nodes representing different region servers. Master servers use these nodes to discover available servers.\n",
    "-   In addition to availability, the nodes are also used to track server failures or network partitions.\n",
    "-   Clients communicate with region servers via Zookeeper.\n",
    "-   In pseudo-distributed and standalone modes, HBase itself will take care of zookeeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache HBase Features\n",
    "\n",
    "Below are the main features provided by Hbase:\n",
    "\n",
    "- HBase is built for low latency operations\n",
    "- HBase provides fast random read operations.  It does so because it uses Hash tables and indexes the data stored in HDFS.\n",
    "- HBase can store large amounts of data easily (terabytes and even petabytes)\n",
    "- Provides scalability within cluster environments\n",
    "- Automatic and configurable sharding (division) of tables\n",
    "- Automatic failover supports between region servers\n",
    "- Convenient base classes available for backing Hadoop MapReduce jobs in HBase tables\n",
    "- Easy to use API for client access\n",
    "- Supports real-time querying efficiently\n",
    "\n",
    "\n",
    "It's also important to note what HBase is __not__:\n",
    "\n",
    "-   It's not a SQL database and doesn't store data using a relational model.\n",
    "-   It's not designed for Online Transaction Processing (OLTP).\n",
    "-   It doesn't provide typical database features like ACID (atomicity, consistency, isolation and durability) or data normalization.\n",
    "-   It's not designed to be used with small datasets.\n",
    "-   Data is sorted _only_ using the row key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and install HBase\n",
    "\n",
    "> It's highly recommended to use a Linux system as HBase is known to cause issues when used with Windows\n",
    "\n",
    "Now that we have a high-level understanding of what HBase is, let's download and install HBase to get a better feel of how it operates. \n",
    "\n",
    "At a high-level, to get HBase working on your system, the steps involve:\n",
    "\n",
    "- Downloading and installing Hadoop (Hadoop's file system is required for HBase in pseudo-distributed mode)\n",
    "- Configuring the following files:\n",
    "    -   bashrc\n",
    "    -   hadoop-env.sh\n",
    "    -   core-site.xml\n",
    "    -   hdfs-site.xml\n",
    "    -   mapred-site-xml\n",
    "    -   yarn-site.xml\n",
    "- Downloading and installing HBase in pseudo-distributed mode (to leverage HDFS for data storage).\n",
    "- Configuring the following HBase files:\n",
    "    -   hbase-env.sh\n",
    "    -   hbase-site.xml\n",
    "\n",
    "__Note__: We'll be using Linux (Ubuntu) for this tutorial, so if you're on a different operating system the steps might differ.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin:\n",
    "\n",
    "1. Open your terminal and run the following commands to __update__ all existing applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt update\n",
    "sudo apt -y upgrade\n",
    "sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ensure that __Java__ is installed on your system. \n",
    "\n",
    "__Note__: It is highly recommended to use `Java 8` as this is the version fully compatible with both Hadoop and HBase. Otherwise, we may face some errors.\n",
    "\n",
    "To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Java version\n",
    "java -version\n",
    "\n",
    "# Check the Java copmiler version\n",
    "javac -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Java is installed correctly, you'll see an output showing the Java and compiler version.  \n",
    "\n",
    "Otherwise, you will need to download and install Java by running the following commands (sometimes both the JRE and JDK are required, so let's go ahead and install both just in case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java 8 runtime environment\n",
    "sudo apt-get install oracle-java8-installer\n",
    "sudo apt-get install openjdk-8-jre \n",
    "\n",
    "# Add Java to the Linux repository\n",
    "sudo add-apt-repository ppa:openjdk-r/ppa\n",
    "sudo apt-get update\n",
    "\n",
    "# Install Java 8 development kit\n",
    "sudo apt-get install openjdk-8-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that both the JDK and the JRE have installed correctly.  If all is well, run the below command and you should see the Java version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ java -version\n",
    "\n",
    "# Output should be similar to this\n",
    "java version \"1.8.0_201\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_201-b09)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javac -version\n",
    "\n",
    "# Output should be similar to this\n",
    "javac 1.8.0_312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you have multiple Java versions installed, it's possible to switch between them.  To do this, check what available Java versions currently exist by running the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Java versions already installed\n",
    "sudo update-alternatives --config java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a menu similar to the below one.  You can select the desired version directly by typing in its number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo update-alternatives --config java\n",
    "[sudo] password for hadoop:\n",
    "\n",
    "# # Output should be similar to this\n",
    "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
    "\n",
    "  Selection\tPath                                        \tPriority   Status\n",
    "------------------------------------------------------------\n",
    "* 0        \t/usr/lib/jvm/java-11-openjdk-amd64/bin/java  \t1111  \tauto mode\n",
    "  1        \t/usr/lib/jvm/java-11-openjdk-amd64/bin/java  \t1111  \tmanual mode\n",
    "  2        \t/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081  \tmanual mode\n",
    "\n",
    "Press <enter> to keep the current choice[*], or type selection number: 2\n",
    "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same step above for the Java compiler `javac`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo update-alternatives --config javac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, we need to ensure that the `JAVA_HOME` variable is correctly set up on your system.  To do that, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variable is correctly set up, you should see a path show up similar to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME\n",
    "\n",
    "# Expected output should be similar to:\n",
    "/usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get no output, or if the output is simply repeating JAVA_HOME, then the variable is not set up correctly.\n",
    "\n",
    "To fix this, you will need to add the correct Java path to your `.bashrc` file.  To do this, first find the location of the Java installation, then open and update the `.bashrc` file to include the following lines at the end of the file (ensure you use the path to your Java folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the .bashrc file using the Nano editor (you can use any other editor like Vim if you prefer)\n",
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the below to the .bashrc file\n",
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export PATH=$PATH:$JAVA_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the file, we need to use the `source` command to enforce the changes in the operating system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make sure everything is set up correctly, check that `JAVA_HOME` is working.  You should now be able to see the Java folder path correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next, we should create a new user account for Hadoop.  HBase uses Hadoop's HDFS to store the data, so it's recommended to have a seperation of accuonts between the Hadoop file system and the Linux file system to avoid confusion.  Go ahead and type the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new user called Hadoop\n",
    "sudo adduser hadoop\n",
    "sudo usermod -aG sudo hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then be asked for some additional information including a password for the new user account.  Go ahead and add that information.  You should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output should be similar to the below\n",
    "Adding user `hadoop' ...\n",
    "Adding new group `hadoop' (1001) ...\n",
    "Adding new user `hadoop' (1001) with group `hadoop' ...\n",
    "Creating home directory `/home/hadoop' ...\n",
    "Copying files from `/etc/skel' ...\n",
    "New password:\n",
    "Retype new password:\n",
    "passwd: password updated successfully\n",
    "Changing the user information for hadoop\n",
    "Enter the new value, or press ENTER for the default\n",
    "    Full Name []: \t \n",
    "    Room Number []:\n",
    "    Work Phone []:\n",
    "    Home Phone []:\n",
    "    Other []:\n",
    "Is the information correct? [Y/n] y\n",
    "adiwany@dodz-vm:~$ sudo usermod -aG sudo hadoop\n",
    "adiwany@dodz-vm:~$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now, we need to generate a __SSH key-pair__ for the new Hadoop user.  Run the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo su hadoop\n",
    "ssh-keygen -t rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should see output similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh-keygen -t rsa\n",
    "\n",
    "# Expected output:\n",
    "Generating public/private rsa key pair.\n",
    "Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):\n",
    "Created directory '/home/hadoop/.ssh'.\n",
    "Enter passphrase (empty for no passphrase):\n",
    "Enter same passphrase again:\n",
    "Your identification has been saved in /home/hadoop/.ssh/id_rsa\n",
    "Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub\n",
    "The key fingerprint is:\n",
    "SHA256:tol1mX4v1GrDLeHHq1Wa/nvKaZUMlEGDCZisTAkWVPc hadoop@dodz-vm\n",
    "The key's randomart image is:\n",
    "+---[RSA 3072]----+\n",
    "|  .=+.o.o.. ++o  |\n",
    "|  .  o.+.  o o.  |\n",
    "|\to .  E  .\t|\n",
    "| \to \to .   |\n",
    "|    \tS +  .o o|\n",
    "|   \t+ =  o .*.|\n",
    "|  \t. o .+.=+. |\n",
    "|       \t.O==..|\n",
    "|       \t..BB=+|\n",
    "+----[SHA256]-----+\n",
    "hadoop@dodz-vm:~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Next, add this newly generated key to the list of authorized SSH keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "sudo chmod 0600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no errors come up, then everything is set up correctly so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Verify that we can use SSH with the newly generated key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything runs smoothly, you should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh localhost\n",
    "\n",
    "# Expected output\n",
    "The authenticity of host 'localhost (127.0.0.1)' can't be established.\n",
    "ECDSA key fingerprint is SHA256:fPYKPrq8VD1pNfI+7EXyKqQFFm4/eWi0+jjADURdHhU.\n",
    "Are you sure you want to continue connecting (yes/no/[fingerprint])? y\n",
    "Please type 'yes', 'no' or the fingerprint: yes\n",
    "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\n",
    "Welcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-41-generic x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com\n",
    " * Management: \thttps://landscape.canonical.com\n",
    " * Support:    \thttps://ubuntu.com/advantage\n",
    "\n",
    "23 updates can be applied immediately.\n",
    "To see these additional updates run: apt list --upgradable\n",
    "\n",
    "Your Hardware Enablement Stack (HWE) is supported until April 2025.\n",
    "\n",
    "The programs included with the Ubuntu system are free software;\n",
    "the exact distribution terms for each program are described in the\n",
    "individual files in /usr/share/doc/*/copyright.\n",
    "\n",
    "Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\n",
    "applicable law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error saying something like __Connection Refused__, then the localhost is not properly set up or SSH is not yet installed.\n",
    "\n",
    "To install SSH, enter the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ssh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ssh\n",
    "\n",
    "# Expected output \n",
    "[sudo] password for hadoop:\n",
    "Reading package lists... Done\n",
    "Building dependency tree  \t \n",
    "Reading state information... Done\n",
    "The following packages were automatically installed and are no longer required:\n",
    "  chromium-codecs-ffmpeg-extra gstreamer1.0-vaapi\n",
    "  libgstreamer-plugins-bad1.0-0 libva-wayland2\n",
    "Use 'sudo apt autoremove' to remove them.\n",
    "The following additional packages will be installed:\n",
    "  ncurses-term openssh-server openssh-sftp-server ssh-import-id\n",
    "Suggested packages:\n",
    "  molly-guard monkeysphere ssh-askpass\n",
    "The following NEW packages will be installed:\n",
    "  ncurses-term openssh-server openssh-sftp-server ssh ssh-import-id\n",
    "0 upgraded, 5 newly installed, 0 to remove and 18 not upgraded.\n",
    "Need to get 693 kB of archives.\n",
    "After this operation, 6,130 kB of additional disk space will be used.\n",
    "Do you want to continue? [Y/n] y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Download and install `Hadoop`\n",
    "\n",
    "Hadoop is required for HBase to correctly work in pseduo-distributed mode.  In this mode, HBase uses Hadoop's file system (HDFS) to store and retrieve the data.\n",
    "\n",
    "To ensure that Hadoop and HBase are compatible, you need to use compatible versions.  For this tutorial, we'll be using the following versions:\n",
    "-   Hadoop 2.10.1\n",
    "-   HBase 1.7.1\n",
    "\n",
    "Save the version to a variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Version=\"2.10.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then download the corresponding Hadoop version as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo wget https://www-eu.apache.org/dist/hadoop/common/hadoop-$Version/hadoop-$Version.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Extract the files and move the resulting folder to the location of your choice (we'll be using `/usr/local/hadoop`).  Create the new directory if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xzvf hadoop-$Version.tar.gz\n",
    "rm hadoop-$Version.tar.gz\n",
    "sudo mv hadoop-$Version/ /usr/local/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now, we need to set `HADOOP_HOME` and add the directory containing the Hadoop binaries to your `.bashrc` file.  To do this, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Hadoop environment variables required to configure the tool on your system.  To do this, we need to add the following content to the end of the `.bashrc` file (remember to use your own Java and Hadoop folder paths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "export HADOOP_HOME=/usr/lib/hadoop/hadoop-2.10.1\n",
    "export HADOOP_INSTALL=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n",
    "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file and exit.  It's vital to run the below command to apply the changes we just added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Next, we'll update the `hadoop-env.sh` file.  This file contains important configurations related to Hadoop's setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the `$JAVA_HOME` variable (i.e., remove the # sign) and _add the full path to the OpenJDK installation on your system without the bin directory_. If you don't know the Java path, run the following command to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readlink -f /usr/bin/javac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your file should look something like this:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hadoop-env3.png\" width=600>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. The `core-site.xml` file defines important HDFS and Hadoop cluster properties.  To set up Hadoop properly, we need to provide the URL of the NameNode (master node).  To do that, run the following command to open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify that we'll be running Hadoop locally, add the following in between the `<configuration>` and `</configuration>` tags and save the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "   <property>\n",
    "      <name>fs.default.name</name>\n",
    "      <value>hdfs://localhost:9000</value>\n",
    "      <description>The default file system URI</description>\n",
    "   </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Next, we need to edit the `hdfs-site.xml` file.  This file stores the details regarding the location of the metadata, NameNode and DataNode directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following properties to the file and, if needed, adjust the NameNode and DataNode directories to your custom locations (create the directories first).  We'll also set the default HDFS replication factor to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "<property>\n",
    "  <name>dfs.name.dir</name>\n",
    "  <value>file:///$HADOOP_HOME/dfsdata/namenode</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>dfs.data.dir</name>\n",
    "  <value>file:///$HADOOP_HOME/dfsdata/datanode</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>dfs.replication</name>\n",
    "  <value>1</value>\n",
    "</property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Edit the `mapred-site.xml` file to define the MapReduce required values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following configuration to change the default MapReduce framework value to `yarn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration> \n",
    "<property> \n",
    "  <name>mapreduce.framework.name</name> \n",
    "  <value>yarn</value> \n",
    "</property> \n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Edit `yarn-site.xml` which is used to define YARN-specific settings by opening the file and adding the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "<property>\n",
    "  <name>yarn.nodemanager.aux-services</name>\n",
    "  <value>mapreduce_shuffle</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the required Hadoop configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Reboot your system to ensure all new settings are loaded before running Hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemctl reboot -i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.  Validate Hadoop settings and configurations\n",
    "\n",
    "After completing the above steps, we want to make sure that Hadoop was properly set up.  To do this, we should do the following to check the Hadoop version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output\n",
    "Hadoop 2.10.1\n",
    "Subversion https://github.com/apache/hadoop -r 1827467c9a56f133025f28557bfc2c562d78e816\n",
    "Compiled by centos on 2020-09-14T13:17Z\n",
    "Compiled with protoc 2.5.0\n",
    "From source with checksum 3114edef868f1f3824e7d0f68be03650\n",
    "This command was run using /usr/lib/hadoop/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Now we are ready to start the Hadoop cluster.  To do this, we need to run a number of commands:\n",
    "\n",
    "- `start-dfs.sh` \n",
    "    -   This command starts HDFS\n",
    "- `start-yarn.sh`\n",
    "    -   This command starts YARN\n",
    "- `jps`\n",
    "    -   This command checks all Java processes to ensure the correct daemons are active\n",
    "\n",
    "__Note__: It's recommended to run these commands from inside your `HADOOP_HOME` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time to start HDFS.  You should see output similar to:\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-hdfs.png\" width=600>\n",
    "</p>\n",
    "\n",
    "\n",
    "Then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash start-yarn.sh\n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the below output:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-yarn-jps.png\" width=600>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If the DataNode process doesn't show up in the jps list, then we probably need to double check the correct HDFS directory is created and that we have the proper persmissions.\n",
    "\n",
    "The commands should be similar to the below (make sure to use your HDFS path as the below is just an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo chmod -R 755 /usr/lib/hadoop/hdfsdata/*\n",
    "sudo chown -R hadoop:hadoop /usr/lib/hadoop/hdfsdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you run `jps`, you should see the DataNode process correctly displayed along with the remaining Hadoop processes:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/datanode-jps.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Another possible error you may encounter is the `incompatible cluster ID error`.  To resolve this, delete both the `datanode` and `namenode` folders and create them again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Use your preferred browser and navigate to your localhost URL or IP. The default port number 9870 gives you access to the Hadoop NameNode user interface, which allows you to monitor the Hadoop enviornment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see a page similar to this one:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hadoop-ui.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can use port 9864 to access the DataNode user interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:9864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And port 8088 can be used to view the YARN Resource Manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Now that Hadoop has been successfully installed, our next objective is to download and setup HBase.  \n",
    "\n",
    "Remember that we'll be using version 1.7.1 as it is compatible with both Hadoop and Java 8.\n",
    "\n",
    "__Note__: Recall that HBase is composed of 3 components:\n",
    "    -   HMaster (coordinating the region server and admin functions)\n",
    "    -   Region Server (maps the region to the server)\n",
    "    -   Zookeeper (coordiantes with Hadoop)\n",
    "\n",
    "We need to see all of these 3 components correctly running to be able to use HBase.\n",
    "\n",
    "Let's start by downloading the HBase installation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Version variable\n",
    "Version=\"1.7.1\"\t\n",
    "# Download the HBase version\n",
    "wget https://dlcdn.apache.org/hbase/Version/hbase-Version-bin.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Extract the downloaded archive and move it to `/usr/local/HBase` (create that folder if it's not already there):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar xvf hbase-$Version-bin.tar.gz\n",
    "sudo mv hbase-$Version/ /usr/local/HBase/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. We need to set your `HBASE_HOME` variable similar to what we did with `JAVA_HOME`.  To do this, copy the path of your HBase folder and open the `bashrc` file to add the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the below lines (use your HBase folder if the path is different):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export HBASE_HOME=/usr/local/hbase/hbase-1.7.1\n",
    "export PATH=$PATH:$HBASE_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Next, we need to update the `hbase-env.sh` file, which contains the configurable paramters for the HBase enviornment.  \n",
    "\n",
    "For running HBase in pseudo-distributed mode, we need to set 3 properties within this file:\n",
    "-   JAVA_HOME\n",
    "-   HBASE_MANAGES_ZOOKEER\n",
    "-   HBASE_REGIONSERVERS\n",
    "\n",
    "Go ahead and open the file and uncomment/add the below settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export HBASE_MANAGES_ZK=true\n",
    "export HBASE_REGIONSERVERS=${HBASE_HOME}/conf/regionservers\n",
    "export JAVA_HOME=${JAVA_HOME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. Then, we will need to update the `hbase-site.xml` file.  Add the following between the `<configuration>` tags (you may need to create the Zookeeper data folder first):\n",
    "\n",
    "Here is what each one of these parameters does:\n",
    "\n",
    "- `hbase.cluster.distributed`\n",
    "    -   This parameter tells HBase to run in a stand-alone local more or on a distributed cluster via Hadoop.\n",
    "\n",
    "- `hbase.tmp.dir`\n",
    "    -   This is the HDFS temporary data storage folder\n",
    "\n",
    "- `hbase.unsafe.stream.capability.enforce`\n",
    "    -   Controls whether or not HBase will check for stream capabilities\n",
    "\n",
    "- `hbase.rootdir`\n",
    "    -   Specifies the the root HDFS folder location\n",
    "\n",
    "- `hbase.zookeeper.property.dataDir`\n",
    "    -   Tells Zookeeper where to store its data files\n",
    "    \n",
    "- `hbase.zookeeper.quorum`\n",
    "    -   This is the list of one or more server nodes that are available for clients requests.  \n",
    "\n",
    "- `dfs.replication`\n",
    "    -   The replication factor for HDFS data (this should match the Hadoop settings we configured earlier)\n",
    "    \n",
    "- `hbase.zookeeper.property.clientPort`\n",
    "    -   Tells Zookeeper which port it should use for communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<property>\n",
    "\t<name>hbase.cluster.distributed</name>\n",
    "\t<value>true</value>\n",
    "</property>\n",
    "<property>\n",
    "\t<name>hbase.tmp.dir</name>\n",
    "\t<value>./tmp</value>\n",
    "</property>\n",
    "<property>\n",
    "\t<name>hbase.unsafe.stream.capability.enforce</name>\n",
    "\t<value>false</value>\n",
    "</property>\n",
    "<property>\n",
    "  \t<name>hbase.rootdir</name>\n",
    "  \t<value>hdfs://localhost:9000/hbase</value>\n",
    "</property>\n",
    "<property>\n",
    "  \t<name>hbase.zookeeper.property.dataDir</name>\n",
    "  \t<value>/usr/lib/hbase/data/zookeeper</value>\n",
    "</property>\n",
    "<property>\n",
    "    \t<name>hbase.zookeeper.quorum</name>\n",
    "    \t<value>localhost</value>\n",
    "</property>\n",
    "<property>\n",
    "    \t<name>dfs.replication</name>\n",
    "    \t<value>1</value>\n",
    "</property>\n",
    "<property>\n",
    "    \t<name>hbase.zookeeper.property.clientPort</name>\n",
    "    \t<value>2181</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Start all the Hadoop daemons first by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HADOOP_HOME/sbin\n",
    "bash start-dfs.sh\n",
    "bash start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything runs smoothly, you should see output similar to the below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/start-dfs-yarn.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything looks good, we'll stop the services and grant the Hadoop user access to Hbase.  Run the below commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop-all.sh\n",
    "chown -R hadoop:root hadoop\n",
    "chmod -R 755 hadoop\n",
    "\n",
    "chown -R hadoop:root Hbase\n",
    "chmod -R 755 Hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. Test HDFS to make sure everything is working smoothly.\n",
    "\n",
    "To do this, we'll create a `test` directory using the below comand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /test\n",
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: The Hadoop file system (HDFS) is _not_ the same as the local file system. In reality, HDFS will be hosted on multiple servers across a distributed network.\n",
    "\n",
    "The output should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -ls /\n",
    "\n",
    "# Expected output\n",
    "2021-12-22 12:27:34,309 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 1 items\n",
    "drwxr-xr-x   - hadoop supergroup      \t0 2021-12-22 12:26 /test\n",
    "hadoop@dodz-vm:/usr/local/hadoop/hadoop-3.3.1/etc/hadoop$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. Next, we'll initiate the HBase process using the provided script `start-hbase.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HBASE_HOME/bin\n",
    "bash start-hbase.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If `permission denied` error shows up, we may need to grant the Hadoop user access to the HBase folders.  To do this, run the below commands (using your correspnoding HBase and Zookeeper paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo chmod -R 755 /usr/lib/hbase/*\n",
    "sudo chown -R hadoop:hadoop /usr/lib/hbase/\n",
    "\n",
    "sudo chmod -R 755 /usr/lib/data/zookeeper/*\n",
    "sudo chown -R hadoop:hadoop /usr/lib/data/zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure all the proper HBase processes are running, run the below Linux command which shows all active processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output should include all of the below processes (3 for HBase and 5 for Hadoop plus jps itself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps\n",
    "\n",
    "# Expected output\n",
    "9298 HMaster\n",
    "5652 SecondaryNameNode\n",
    "5286 NameNode\n",
    "9238 HQuorumPeer\n",
    "9399 HRegionServer\n",
    "5784 ResourceManager\n",
    "6684 DataNode\n",
    "5918 NodeManager\n",
    "9486 Jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the required processes run, we now need to run the HBase shell to ensure that we can start interacting with HBase.\n",
    "\n",
    "To do this, run the below command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be inside the HBase shell as we can see below:  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/hbase-shell.png\" width=600>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are inside HBase and can begin to use it's commands.\n",
    "\n",
    "Try to run the `status` command to ensure HBase is working successfully.  This command shows the list of active HBase servers. \n",
    "The output should be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbase(main):001:0> status\n",
    "\n",
    "#Expected output\n",
    "1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load\n",
    "\n",
    "hbase(main):002:0>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error that mentions HMaster is not running, double check the `/etc/hosts` file to ensure the VM and the localhost both have the same IP (127.0.0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nano /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. Now that HBase is up and running, we'll proceed to create a table and populate it with data.\n",
    "\n",
    "The first step is to [download the Employee data file from here](LINK).\n",
    "\n",
    "__Note__: If you are using a Virtual Machine, try to download the file directly in the VM rather than the host operating system.  This avoids having to do a file transfer between the two systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.The next step is to import the Employee data file into HBase.\n",
    "\n",
    "To do that, we need to create an Hbase table and specify the Column Family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create 'emp_data',{NAME => 'cf'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the table was created successfully, run the `list` command to see all available HBase tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list\n",
    "\n",
    "#Expected output:\n",
    "hbase(main):002:0> list\n",
    "emp_data                                                                         \t \n",
    "1 row(s) in 0.3180 seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the table is created, we need to run the below command to copy the CSV file to HDFS so we can import it into HBase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=',' -Dimporttsv.columns=HBASE_ROW_KEY,cf:ename,cf:designation,cf:manager,cf:hire_date,cf:sal,cf:deptno emp_data /data/emp_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you get any errors, such as \"Bad Lines\" or \"Failed Map\", check that you didn't miss any charactars from the above code and attempt to type it directly yourself instead of copy and pasting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works smoothly, you should see output similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2022-01-07 13:34:28,910 INFO  [main] mapreduce.Job: erations=0\n",
    "   \t HDFS: Number of bytes read=756\n",
    "   \t HDFS: Number of bytes written=0\n",
    "   \t HDFS: Number of read operations=2\n",
    "   \t HDFS: Number of large read operations=0\n",
    "   \t HDFS: Number of write operations=0\n",
    "    Job Counters\n",
    "   \t Launched map tasks=1\n",
    "   \t Data-local map tasks=1\n",
    "   \t Total time spent by all maps in occupied slots (ms)=5154\n",
    "   \t Total time spent by all reduces in occupied slots (ms)=0\n",
    "   \t Total time spent by all map tasks (ms)=5154\n",
    "   \t Total vcore-milliseconds taken by all map tasks=5154\n",
    "   \t Total megabyte-milliseconds taken by all map tasks=5277696\n",
    "    Map-Reduce Framework\n",
    "   \t Map input records=15\n",
    "   \t Map output records=15\n",
    "   \t Input split bytes=104\n",
    "   \t Spilled Records=0\n",
    "   \t Failed Shuffles=0\n",
    "   \t Merged Map outputs=0\n",
    "   \t GC time elapsed (ms)=77\n",
    "   \t CPU time spent (ms)=1600\n",
    "   \t Physical memory (bytes) snapshot=183992320\n",
    "   \t Virtual memory (bytes) snapshot=1874804736\n",
    "   \t Total committed heap usage (bytes)=137953280\n",
    "    ImportTsv\n",
    "   \t Bad Lines=0\n",
    "    File Input Format Counters\n",
    "   \t Bytes Read=652\n",
    "    File Output Format Counters\n",
    "   \t Bytes Written=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. Now, we need to go into the HBase shell and check that the data is correctly loaded.  To do that, we'll use the `scan` command (which is similar to a SQL SELECT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan 'emp_data'\n",
    "\n",
    "# Expected output:\n",
    "hbase(main):004:0> scan 'emp_data'\n",
    "ROW                \tCOLUMN+CELL                                               \t \n",
    " 7369              \tcolumn=cf:deptno, timestamp=1641555244509, value=20       \t \n",
    " 7369              \tcolumn=cf:designation, timestamp=1641555244509, value=CLERK    \n",
    " 7369              \tcolumn=cf:ename, timestamp=1641555244509, value=SMITH     \t \n",
    " 7369              \tcolumn=cf:hire_date, timestamp=1641555244509, value=12/17/1980\n",
    " 7369              \tcolumn=cf:manager, timestamp=1641555244509, value=7902    \t \n",
    " 7369              \tcolumn=cf:sal, timestamp=1641555244509, value=800         \t \n",
    " 7499              \tcolumn=cf:deptno, timestamp=1641555244509, value=30       \t \n",
    " 7499              \tcolumn=cf:designation, timestamp=1641555244509, value=SALESMAN\n",
    " 7499              \tcolumn=cf:ename, timestamp=1641555244509, value=ALLEN     \t \n",
    " 7499              \tcolumn=cf:hire_date, timestamp=1641555244509, value=2/20/1981  \n",
    " 7499              \tcolumn=cf:manager, timestamp=1641555244509, value=7698    \t \n",
    " 7499              \tcolumn=cf:sal, timestamp=1641555244509, value=1600        \t \n",
    " 7521              \tcolumn=cf:deptno, timestamp=1641555244509, value=30       \t \n",
    " 7521              \tcolumn=cf:designation, timestamp=1641555244509, value=SALESMAN\n",
    " 7521              \tcolumn=cf:ename, timestamp=1641555244509, value=WARD      \t \n",
    " 7521              \tcolumn=cf:hire_date, timestamp=1641555244509, value=2/22/1981  \n",
    " 7521              \tcolumn=cf:manager, timestamp=1641555244509, value=7698    \t \n",
    " 7521              \tcolumn=cf:sal, timestamp=1641555244509, value=1250        \t \n",
    " 7566              \tcolumn=cf:deptno, timestamp=1641555244509, value=20       \t \n",
    " 7566              \tcolumn=cf:designation, timestamp=1641555244509, value=MANAGER  \n",
    " 7566              \tcolumn=cf:ename, timestamp=1641555244509, value=TURNER    \t \n",
    " 7566              \tcolumn=cf:hire_date, timestamp=1641555244509, value=4/2/1981   \n",
    " 7566              \tcolumn=cf:manager, timestamp=1641555244509, value=7839    \t \n",
    " 7566              \tcolumn=cf:sal, timestamp=1641555244509, value=2975        \t \n",
    " 7654              \tcolumn=cf:deptno, timestamp=1641555244509, value=30       \t \n",
    " 7654              \tcolumn=cf:designation, timestamp=1641555244509, value=SALESMAN\n",
    " 7654              \tcolumn=cf:ename, timestamp=1641555244509, value=MARTIN    \t \n",
    " 7654              \tcolumn=cf:hire_date, timestamp=1641555244509, value=9/28/1981  \n",
    " 7654              \tcolumn=cf:manager, timestamp=1641555244509, value=7698    \t \n",
    " 7654              \tcolumn=cf:sal, timestamp=1641555244509, value=1250        \t \n",
    " 7698              \tcolumn=cf:deptno, timestamp=1641555244509, value=30       \t \n",
    " 7698              \tcolumn=cf:designation, timestamp=1641555244509, value=MANAGER  \n",
    " 7698              \tcolumn=cf:ename, timestamp=1641555244509, value=MILLER    \t \n",
    " 7698              \tcolumn=cf:hire_date, timestamp=1641555244509, value=5/1/1981   \n",
    " 7698              \tcolumn=cf:manager, timestamp=1641555244509, value=7839    \t \n",
    " 7698              \tcolumn=cf:sal, timestamp=1641555244509, value=2850        \t \n",
    " 7782              \tcolumn=cf:deptno, timestamp=1641555244509, value=10       \t \n",
    " 7782              \tcolumn=cf:designation, timestamp=1641555244509, value=MANAGER  \n",
    " 7782              \tcolumn=cf:ename, timestamp=1641555244509, value=CLARK     \t \n",
    " 7782              \tcolumn=cf:hire_date, timestamp=1641555244509, value=6/9/1981   \n",
    " 7782              \tcolumn=cf:manager, timestamp=1641555244509, value=7839    \t \n",
    " 7782              \tcolumn=cf:sal, timestamp=1641555244509, value=2450        \t \n",
    " 7788              \tcolumn=cf:deptno, timestamp=1641555244509, value=20       \t \n",
    " 7788              \tcolumn=cf:designation, timestamp=1641555244509, value=ANALYST  \n",
    " 7788              \tcolumn=cf:ename, timestamp=1641555244509, value=SCOTT     \t \n",
    " 7788              \tcolumn=cf:hire_date, timestamp=1641555244509, value=12/9/1982  \n",
    " 7788              \tcolumn=cf:manager, timestamp=1641555244509, value=7566    \t \n",
    " 7788              \tcolumn=cf:sal, timestamp=1641555244509, value=3000        \t \n",
    " 7839              \tcolumn=cf:deptno, timestamp=1641555244509, value=10       \t \n",
    " 7839              \tcolumn=cf:designation, timestamp=1641555244509, value=PRESIDENT\n",
    " 7839              \tcolumn=cf:ename, timestamp=1641555244509, value=KING      \t \n",
    " 7839              \tcolumn=cf:hire_date, timestamp=1641555244509, value=11/17/1981\n",
    " 7839              \tcolumn=cf:manager, timestamp=1641555244509, value=NULL    \t \n",
    " 7839              \tcolumn=cf:sal, timestamp=1641555244509, value=5000        \t \n",
    " 7844              \tcolumn=cf:deptno, timestamp=1641555244509, value=30       \t \n",
    " 7844              \tcolumn=cf:designation, timestamp=1641555244509, value=SALESMAN\n",
    " 7844              \tcolumn=cf:ename, timestamp=1641555244509, value=TURNER    \t \n",
    " 7844              \tcolumn=cf:hire_date, timestamp=1641555244509, value=9/8/1981   \n",
    " 7844              \tcolumn=cf:manager, timestamp=1641555244509, value=7698    \t \n",
    " 7844              \tcolumn=cf:sal, timestamp=1641555244509, value=1500        \t \n",
    " 7876              \tcolumn=cf:deptno, timestamp=1641555244509, value=20       \t \n",
    " 7876              \tcolumn=cf:designation, timestamp=1641555244509, value=CLERK    \n",
    " 7876              \tcolumn=cf:ename, timestamp=1641555244509, value=ADAMS     \t \n",
    " 7876              \tcolumn=cf:hire_date, timestamp=1641555244509, value=1/12/1983  \n",
    " 7876              \tcolumn=cf:manager, timestamp=1641555244509, value=7788    \t \n",
    " 7876              \tcolumn=cf:sal, timestamp=1641555244509, value=1100        \t \n",
    " 7900              \tcolumn=cf:deptno, timestamp=1641555244509, value=30       \t \n",
    " 7900              \tcolumn=cf:designation, timestamp=1641555244509, value=CLERK    \n",
    " 7900              \tcolumn=cf:ename, timestamp=1641555244509, value=JAMES     \t \n",
    " 7900              \tcolumn=cf:hire_date, timestamp=1641555244509, value=12/3/1981  \n",
    " 7900              \tcolumn=cf:manager, timestamp=1641555244509, value=7698    \t \n",
    " 7900              \tcolumn=cf:sal, timestamp=1641555244509, value=950         \t \n",
    " 7902              \tcolumn=cf:deptno, timestamp=1641555244509, value=20       \t \n",
    " 7902              \tcolumn=cf:designation, timestamp=1641555244509, value=ANALYST  \n",
    " 7902              \tcolumn=cf:ename, timestamp=1641555244509, value=FORD      \t \n",
    " 7902              \tcolumn=cf:hire_date, timestamp=1641555244509, value=12/3/1981  \n",
    " 7902              \tcolumn=cf:manager, timestamp=1641555244509, value=7566    \t \n",
    " 7902              \tcolumn=cf:sal, timestamp=1641555244509, value=3000        \t \n",
    " 7934              \tcolumn=cf:deptno, timestamp=1641555244509, value=10       \t \n",
    " 7934              \tcolumn=cf:designation, timestamp=1641555244509, value=CLERK    \n",
    " 7934              \tcolumn=cf:ename, timestamp=1641555244509, value=MILLER    \t \n",
    " 7934              \tcolumn=cf:hire_date, timestamp=1641555244509, value=1/23/1982  \n",
    " 7934              \tcolumn=cf:manager, timestamp=1641555244509, value=7782    \t \n",
    " 7934              \tcolumn=cf:sal, timestamp=1641555244509, value=1300        \t \n",
    " empno             \tcolumn=cf:deptno, timestamp=1641555244509, value=deptno   \t \n",
    " empno             \tcolumn=cf:designation, timestamp=1641555244509, value=designati\n",
    " empno             \tcolumn=cf:ename, timestamp=1641555244509, value=ename     \t \n",
    " empno             \tcolumn=cf:hire_date, timestamp=1641555244509, value=hire_date  \n",
    " empno             \tcolumn=cf:manager, timestamp=1641555244509, value=manager \t \n",
    " empno             \tcolumn=cf:sal, timestamp=1641555244509, value=sal         \t \n",
    "15 row(s) in 0.5130 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. Take a detailed look at how the data is displayed in HBase as it may seem confusing at first.  Unlike a relational database which stores data in a row-based manner, HBase stores the data in a __column-based__ approach. \n",
    "\n",
    "Each line in HBase represents a column value and also includes an automatic timestamp. The __Row__ is a unique Rowkey identifier that tells HBase how each of the columns are connected to each other (i.e. if they are part of the same logical row or not).\n",
    "\n",
    "Once you feel you have a good sense of how the data is structured in HBase, let's go ahead and look at some HBase commands we can use to interact with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBase commands:\n",
    "\n",
    "Below are some of the typical commands you would be using to interact with data in HBase:\n",
    "\n",
    "- `put`\n",
    "    -   This command allows you to update the data in an already existing cell.\n",
    "\n",
    "- `get`\n",
    "    -   This command are used to read data from a table in HBase. It returns a the values associated with a row of data at a time.\n",
    "\n",
    "- `delete`\n",
    "    -   This command allows you to delete a specific cell in an HBase table.\n",
    "\n",
    "- `deleteall`\n",
    "    -   This command deletes all of the cells in a table.\n",
    "\n",
    "- `scan`\n",
    "    -   This command is used to view the data stored in an HBase table.\n",
    "\n",
    "- `count`\n",
    "    -   This command is used to count the number of rows of a table.\n",
    "\n",
    "- `disable`\n",
    "    -   This command disables (turns off) a table so that it can be deleted.\n",
    "\n",
    "- `drop`\n",
    "    -   This commands deletes a disabled table.\n",
    "\n",
    "-   `truncate`\n",
    "    -   This commands does 3 things in sequence:\n",
    "        -   Disables a table\n",
    "        -   Drops a table\n",
    "        -   Recreates the table with the same name\n",
    "\n",
    "\n",
    "For a detailed explanation of HBase commands, check the following guide:\n",
    "-    [HBase Cheat Sheet](https://sparkbyexamples.com/hbase/hbase-shell-commands-cheat-sheet/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- HBase is a modern tool for storing and analyzing big data in tables.  It does so using a column-oriented approach.  This should not be confused with the row-oriented approach that traditional relational databases use.\n",
    "- The intersection of a row and column in a table is called a _cell_.  Cells store data, which in turn is accessed using a unique ID called the _row key_.\n",
    "- Related columns in HBase are grouped together into _column families_. An HBase table can have more than one column family. \n",
    "- HBase's architecture is composed of 3 main components: _HMaster_ (which acts as the master server), _Region Servers_ (which are various nodes that store tables), and _Zookeeper_ (which coordinates the various administrative tasks).\n",
    "- HBase is designed to efficiently handle unstructured and semi-structured data using low-latency operations.  The tool is easy to scale and support batch and real-time querying of data.\n",
    "- Hbase can be installed in different modes including stand-alone (on a local machine), pseudo-distributed (using Hadoop as the underlying data store) and fully distributed (across a corporate cluster).\n",
    "- To download and install HBase in pseudo-distributed mode, we'll need to have a compatible Java and Hadoop version installed beforehand. Using a Linux operating system is highly recommended.\n",
    "- Tables in HBase can be created using the `create` command.  Table querying can be done using `scan` and `get` commands, while inserting data can be done using the `put` command.\n",
    "- In order to delete an HBase table, we first need to `disable` the table and then `drop` the disabled table.  Alternatively, the `truncate` command can be used to implement all of these actions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
